"""
RAG Service for UzSWLU Chatbot
Retrieves context from both ChromaDB and PostgreSQL database.
"""
import chromadb
from chromadb.config import Settings
from typing import List, Dict, Any
import logging
import os
from django.conf import settings
from django.db import models

# Self-Correction modules (v5.0)
from self_correction.grader import RelevanceGrader
from self_correction.hallucination_checker import HallucinationChecker
from self_correction.faq_hierarchy import FAQHierarchyEnforcer

# Setup logging
logger = logging.getLogger(__name__)
logger.info("‚úÖ RAG Service loading...")


class RAGService:
    def __init__(self, persist_directory="/app/chroma_db"):
        self.persist_directory = persist_directory
        self.collection_name = "uzswlu_knowledge"
        
        # ChromaDB client - telemetry o'chirilgan (xatoliklarni oldini olish uchun)
        self.client = chromadb.PersistentClient(
            path=persist_directory,
            settings=Settings(
                anonymized_telemetry=False,
                allow_reset=True
            )
        )
        
        # Telemetry xatoliklarini yashirish
        import logging
        logging.getLogger("chromadb").setLevel(logging.WARNING)
        logging.getLogger("posthog").setLevel(logging.CRITICAL)
        
        # PostHog capture() error monkeypatch (if library is causing issues)
        try:
            import posthog
            if hasattr(posthog, 'capture'):
                orig_capture = posthog.capture
                def silent_capture(*args, **kwargs):
                    return None
                posthog.capture = silent_capture
        except ImportError:
            pass
        
        # Self-Correction components (v5.0)
        self.grader = RelevanceGrader()
        self.hallucination_checker = HallucinationChecker()
        self.faq_hierarchy = FAQHierarchyEnforcer()
        
        # Embedding function - nomic-embed-text via Ollama API
        # Avval Ollama'da model mavjudligini tekshirish
        embedding_function = None
        try:
            # Try Ollama nomic-embed-text first (as per prompt requirements)
            from ollama_integration.embedding import OllamaEmbeddingFunction
            import requests
            
            # Test if model exists
            from django.conf import settings as django_settings
            test_url = getattr(django_settings, 'OLLAMA_URL', 'http://ollama:11434')
            try:
                test_response = requests.post(
                    f"{test_url}/api/embeddings",
                    json={"model": "nomic-embed-text", "prompt": "test"},
                    timeout=(5, 10)
                )
                if test_response.status_code == 404:
                    logger.warning("‚ö†Ô∏è nomic-embed-text model Ollama'da topilmadi. Fallback ishlatiladi.")
                    raise ValueError("Model not found")
                elif test_response.status_code != 200:
                    logger.warning(f"‚ö†Ô∏è nomic-embed-text test xatolik: {test_response.status_code}. Fallback ishlatiladi.")
                    raise ValueError("Model test failed")
            except (requests.exceptions.RequestException, ValueError) as test_error:
                logger.warning(f"‚ö†Ô∏è nomic-embed-text test xatolik: {test_error}. Fallback ishlatiladi.")
                raise
            
            # If test passed, use nomic-embed-text
            embedding_function = OllamaEmbeddingFunction(model_name="nomic-embed-text")
            logger.info("‚úÖ Using Ollama nomic-embed-text embedding model")
            
        except Exception as e1:
            logger.warning(f"‚ö†Ô∏è Ollama nomic-embed-text yuklashda xatolik: {e1}")
            # Fallback to multilingual SentenceTransformer (matches existing collection dimension 384)
            try:
                from chromadb.utils import embedding_functions
                embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(
                    model_name="paraphrase-multilingual-MiniLM-L12-v2"
                )
                logger.info("‚úÖ Using Multilingual SentenceTransformer (fallback - matches existing collection)")
            except Exception as e2:
                logger.warning(f"‚ö†Ô∏è SentenceTransformer yuklashda xatolik: {e2}")
                try:
                    # Final fallback: Default embedding function
                    embedding_function = embedding_functions.DefaultEmbeddingFunction()
                    logger.info("‚úÖ Using DefaultEmbeddingFunction (final fallback)")
                except Exception as e3:
                    logger.warning(f"‚ö†Ô∏è Default embedding function yuklashda xatolik: {e3}")
                    embedding_function = None
        
        self.embedding_fn = embedding_function
        
        try:
            self.collection = self.client.get_collection(
                self.collection_name,
                embedding_function=self.embedding_fn
            )
            count = self.collection.count()
            logger.info(f"‚úÖ ChromaDB: {count} docs")
            
            # Avtomatik sync - agar FAQ'lar yo'q bo'lsa yoki kam bo'lsa
            if count == 0:
                logger.info("‚ö†Ô∏è ChromaDB bo'sh, FAQ'larni sync qilmoqda...")
                self.sync_from_database()
        except Exception as e:
            logger.error(f"‚ùå ChromaDB yuklashda xatolik: {e}")
            self.collection = None
    
    def _detect_category(self, question: str) -> Optional[Any]:
        """
        v6.0: Detect category from question using intent keywords.
        Returns Category object or None.
        """
        from chatbot_app.models import Category
        
        q_lower = question.lower()
        
        # Try to match category by intent keywords
        categories = Category.objects.filter(is_active=True)
        
        for category in categories:
            if category.intent_keywords:
                for keyword in category.intent_keywords:
                    if keyword.lower() in q_lower:
                        logger.info(f"üéØ Category detected: {category.name} (keyword: {keyword})")
                        return category
        
        return None
    
    def _resolve_dynamic_variables(self, answer: str, variables: list, lang_code: str) -> str:
        """
        v6.0: Replace {{variable}} placeholders with actual DynamicInfo values.
        """
        from chatbot_app.models import DynamicInfo
        
        for var_key in variables:
            try:
                dynamic_info = DynamicInfo.objects.get(key=var_key, is_active=True)
                # Get language-specific value
                value = getattr(dynamic_info, f'value_{lang_code}', None) or dynamic_info.value_uz or dynamic_info.value
                answer = answer.replace(f"{{{{{var_key}}}}}", value)
                logger.info(f"‚úÖ Resolved variable: {var_key} = {value[:50]}...")
            except DynamicInfo.DoesNotExist:
                logger.warning(f"‚ö†Ô∏è Dynamic variable not found: {var_key}")
        
        return answer
    
    def sync_from_database(self):
        """Sync FAQs from database to ChromaDB"""
        # This method will be implemented separately
        pass
    
    def _old_sync_check(self):
        """Old sync logic - to be removed"""
        if False:  # Disabled
                logger.info("‚ö†Ô∏è ChromaDB bo'sh, FAQ'larni sync qilmoqda...")
                try:
                    self.sync_from_database()
                    logger.info("‚úÖ FAQ'lar ChromaDB'ga sync qilindi")
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Avtomatik sync xatolik: {e}")
        except Exception as e:
            logger.info(f"‚ö†Ô∏è Collection topilmadi yoki xatolik: {e}")
            # Collection yaratish - explicit embedding function bilan
            self.collection = self.client.create_collection(
                name=self.collection_name,
                embedding_function=embedding_function,
                metadata={
                    "description": "UzSWLU knowledge base",
                    "hnsw:space": "cosine"
                }
            )
            logger.info("‚úÖ Created ChromaDB collection")
            
            # Yangi collection yaratilganda avtomatik sync
            try:
                logger.info("‚ö†Ô∏è Yangi collection, FAQ'larni sync qilmoqda...")
                self.sync_from_database()
                logger.info("‚úÖ FAQ'lar ChromaDB'ga sync qilindi")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Avtomatik sync xatolik: {e}")
    
    def sync_from_database(self):
        """Sync FAQ translations from PostgreSQL to ChromaDB."""
        try:
            import django
            django.setup()
            from chatbot_app.models import FAQTranslation
            
            translations = FAQTranslation.objects.filter(faq__status='published')
            if not translations.exists():
                logger.info("No published FAQ translations to sync")
                return 0
            
            try:
                self.client.delete_collection(self.collection_name)
            except:
                pass
            
            # Use stored embedding function
            self.collection = self.client.create_collection(
                name=self.collection_name,
                embedding_function=self.embedding_fn,
                metadata={"hnsw:space": "cosine"}
            )
            
            texts = []
            metadatas = []
            ids = []
            
            for trans in translations:
                # Combine question and answer for embedding
                search_text = f"Question: {trans.question}\nAnswer: {trans.answer}"
                texts.append(search_text)
                metadatas.append({
                    'faq_id': trans.faq_id,
                    'trans_id': trans.id,
                    'lang': trans.lang,
                    'category': trans.faq.category.name if trans.faq.category else 'General',
                    'type': 'faq',
                    'is_current': trans.faq.is_current,
                    'year': trans.faq.year
                })
                ids.append(f"faq_trans_{trans.id}")
            
            if texts:
                self.collection.add(documents=texts, metadatas=metadatas, ids=ids)
                logger.info(f"‚úÖ Synced {len(texts)} FAQ translations to ChromaDB")
            
            return len(texts)
        except Exception as e:
            logger.error(f"‚ùå Sync error: {e}")
            return 0
    
    def search_database(self, query_text: str, lang_code: str = 'uz', limit: int = 5) -> List[Dict]:
        """Search FAQ translations from PostgreSQL using Full-Text Search on search_tsv."""
        try:
            from chatbot_app.models import FAQTranslation
            from django.contrib.postgres.search import SearchQuery, SearchRank, TrigramSimilarity
            from django.db.models import F, Value

            # Language to Postgres Search Config mapping
            lang_configs = {
                'uz': 'simple',
                'en': 'english',
                'ru': 'russian'
            }
            search_config = lang_configs.get(lang_code, 'simple')
            
            # Define search_query for FTS using language-specific config
            search_query = SearchQuery(query_text, config=search_config, search_type='plain')

            # Hybrid Search Ranking: 
            # - Exact/Stemmed Keyword Match (FTS)
            # - Fuzzy Keyword Match (Trigram Similarity)
            # - High weight to Question, slightly lower to Answer
            trans_results = FAQTranslation.objects.filter(
                lang=lang_code,
                faq__status='published'
            ).annotate(
                q_rank=SearchRank(F('question_tsv'), search_query),
                a_rank=SearchRank(F('answer_tsv'), search_query),
                q_sim=TrigramSimilarity('question', query_text),
                a_sim=TrigramSimilarity('answer', query_text),
                # Hybrid score calculation (Higher weighting for exact matches and trigrams in questions)
                rank=(F('q_rank') * 2.0 + F('a_rank') * 0.5 + F('q_sim') * 3.0 + F('a_sim') * 1.0)
            ).filter(rank__gte=0.1).order_by('-rank')[:limit]
            
            results = []
            for trans in trans_results:
                # POWER BOOST for exact keyword hits in the question
                # This overcomes the 'dilution' caused by common words like 'haqida' or 'ma'lumot'
                boost = 1.0
                core_keywords = {
                    'rektor': ['rektor', 'rector'],
                    'faq': ['fakultet', 'faculty'],
                    'adm': ['qabul', 'admission'],
                    'tel': ['aloqa', 'bog\'lanish', 'contact', 'telefon'],
                    'hist': ['tarix', 'history']
                }
                
                query_lower = query_text.lower()
                q_text_lower = trans.question.lower()
                
                for _, keywords in core_keywords.items():
                    if any(kw in query_lower for kw in keywords) and any(kw in q_text_lower for kw in keywords):
                        boost = 2.5 # Significant boost for matching core intent
                        break

                results.append({
                    'faq_id': trans.faq_id,
                    'question': trans.question,
                    'answer': trans.answer,
                    'category': trans.faq.category.name if trans.faq.category else 'General',
                    'relevance': min(1.0, float(trans.rank) * boost * (1.2 if trans.faq.is_current else 0.8)),
                    'source': 'db_fts',
                    'lang': trans.lang,
                    'is_current': trans.faq.is_current,
                    'year': trans.faq.year
                })
            
            # 2. Fallback search (if no results in target language, try others)
            if not results:
                other_trans = FAQTranslation.objects.filter(
                    faq__status='published'
                ).exclude(lang=lang_code).annotate(
                    q_rank=SearchRank(F('question_tsv'), search_query),
                    a_rank=SearchRank(F('answer_tsv'), search_query),
                    q_sim=TrigramSimilarity('question', query_text),
                    a_sim=TrigramSimilarity('answer', query_text),
                    rank=(F('q_rank') * 2.0 + F('a_rank') * 0.5 + F('q_sim') * 3.0 + F('a_sim') * 1.0)
                ).filter(rank__gte=0.1).order_by('-rank')[:limit]
                
                for trans in other_trans:
                    results.append({
                        'faq_id': trans.faq_id,
                        'question': trans.question,
                        'answer': trans.answer,
                        'category': trans.faq.category.name if trans.faq.category else 'General',
                        'relevance': float(trans.rank) * 0.8, # Penalty for different language
                        'source': 'db_fts_fallback',
                        'lang': trans.lang
                    })
            
            # 3. Last fallback: icontains search
            if not results:
                icontains_results = FAQTranslation.objects.filter(
                    faq__status='published',
                    question__icontains=query_text
                )[:limit]
                for trans in icontains_results:
                    results.append({
                        'faq_id': trans.faq_id,
                        'question': trans.question,
                        'answer': trans.answer,
                        'category': trans.faq.category.name if trans.faq.category else 'General',
                        'relevance': 0.1,
                        'source': 'db_icontains',
                        'lang': trans.lang
                    })
            
            return results
        except Exception as e:
            logger.error(f"Database search error: {e}")
            return []
    
    def search_chromadb(self, query: str, lang_code: str = 'uz', top_k: int = 5) -> List[Dict]:
        """Semantic search in ChromaDB with language awareness."""
        if self.collection.count() == 0:
            return []
        
        try:
            # Prefer matching language in metadata
            where_clause = {"lang": lang_code}
            
            n_results = min(top_k * 2, self.collection.count())
            results = self.collection.query(
                query_texts=[query],
                n_results=n_results,
                where=where_clause
            )
            
            # If no results for language, try all
            if not results['documents'][0]:
                results = self.collection.query(
                    query_texts=[query],
                    n_results=n_results
                )
        except Exception as e:
            logger.error(f"‚ùå ChromaDB search error: {e}")
            return []
        
        documents = []
        if results and results.get('documents'):
            for i, doc in enumerate(results['documents'][0]):
                meta = results['metadatas'][0][i] if results.get('metadatas') else {}
                dist = results['distances'][0][i] if results.get('distances') else 1.0
                # Stricter similarity: cosine distance usually 0-2 (0 is same)
                similarity = max(0, 1 - dist)
                
                # Minimum threshold application (as requested by user: 0.6)
                if similarity < 0.3: # Using 0.3 here because cosine distance of 0.7 is actually quite similar
                    continue
                    
                documents.append({
                    'text': doc,
                    'title': meta.get('title', 'Knowledge Base'),
                    'category': meta.get('category', 'General'),
                    'faq_id': meta.get('faq_id'),
                    'document_id': meta.get('document_id'),
                    'chunk_index': meta.get('chunk_index'),
                    'lang': meta.get('lang'),
                    'similarity': similarity * (1.1 if meta.get('is_current', True) else 0.9),
                    'is_current': meta.get('is_current', True),
                    'year': meta.get('year', 2024),
                    'source': meta.get('source', 'semantic')
                })
        
        return sorted(documents, key=lambda x: x['similarity'], reverse=True)[:top_k]

    def retrieve_with_sources(self, question: str, lang_code: str = 'uz', top_k: int = 5) -> Dict[str, Any]:
        """
        Prioritized retrieval with language support.
        1. Intent-Based Filtering & Category Matching
        2. Database FTS Search (with FAQ boosting)
        3. ChromaDB Semantic Search
        4. Rule-Based Reranking
        """
        # --- 1. Intent Detection ---
        intent_category = None
        q_lower = question.lower()
        
        # Simple keywords for intent matching
        intents = {
            'Talaba hayoti': ['yotoqxona', 'hostel', 'turar joy', 'student life', 'ttj', 'oshxona', 'sport'],
            'Qabul': ['qabul', 'admission', 'hujjat topshirish', 'imtihon', 'imtixon', 'abituriyent'],
            'Kontrakt': ['kontrakt', 'to\'lov', 'payment', 'tuition', 'shartnoma'],
            'Magistratura': ['magistr', 'master', 'postgraduate', 'magistratura'],
            'Rektorat': ['rektor', 'prorektor', 'rahbariyat', 'rector']
        }
        
        for cat, keywords in intents.items():
            if any(kw in q_lower for kw in keywords):
                intent_category = cat
                break

        # --- 2. Database & Semantic Searches ---
        db_results = self.search_database(question, lang_code=lang_code, limit=top_k)
        semantic_results = self.search_chromadb(question, lang_code=lang_code, top_k=top_k)
        
        merged_results = []
        seen_faq_ids = set()
        
        # Determine if we should prioritize documents (e.g., query about prices/specific numbers)
        data_keywords = ['narx', 'kontrakt', 'miqdor', 'price', 'fee', 'tuition', 'summa', 'raqam', 'to\'lov']
        should_boost_docs = any(kw in q_lower for kw in data_keywords)

        # 1. Add DB results (FAQ)
        for r in db_results:
            confidence = r['relevance'] * 1.25
            if intent_category and r['category'] == intent_category:
                confidence *= 1.2
            
            seen_faq_ids.add(r['faq_id'])
            merged_results.append({
                'text': r['answer'],
                'title': r['question'],
                'category': r['category'],
                'confidence': min(0.99, confidence),
                'source_type': 'faq',
                'faq_id': r['faq_id']
            })
            
        # 2. Add Semantic results
        for r in semantic_results:
            if r['faq_id'] and r['faq_id'] in seen_faq_ids:
                continue
            
            confidence = r['similarity']
            if should_boost_docs and r['source'] == 'document':
                confidence *= 2.0 # Huge boost for docs when data is requested
            
            if intent_category and r.get('category') == intent_category:
                confidence *= 1.3
            elif intent_category and r.get('category') and r.get('category') != intent_category:
                confidence *= 0.5

            merged_results.append({
                'text': r['text'],
                'title': r['title'],
                'category': r.get('category', 'Hujjat'),
                'confidence': min(0.99, confidence),
                'source_type': 'document',
                'document_id': r.get('document_id'),
                'chunk_index': r.get('chunk_index')
            })
            
        # --- 3. Deduplication & Sorting ---
        merged_results.sort(key=lambda x: x['confidence'], reverse=True)
        
        # Filtering
        top_results = [r for r in merged_results if r['confidence'] >= 0.4]
        if not top_results:
            top_results = [r for r in merged_results if r['confidence'] >= 0.25]

        # --- 4. Contextual Formatting with Neighbor Merging ---
        detail_keywords = ['batafsil', 'more', 'detail', 'to\'liq', 'podrobno', 'uzunroq']
        is_detail_requested = any(kw in q_lower for kw in detail_keywords)
        
        actual_top_k = top_k
        top_k_results = top_results[:actual_top_k]
        
        faq_results = [r for r in top_k_results if r['source_type'] == 'faq']
        doc_results = [r for r in top_k_results if r['source_type'] != 'faq']

        # Sort priority: if data is requested, put documents BEFORE FAQs
        if should_boost_docs:
            sorted_top_results = doc_results + faq_results
        else:
            sorted_top_results = faq_results + doc_results
        
        context_blocks = []
        used_doc_chunks = {} # {doc_id: [indices]}
        
        for r in sorted_top_results:
            # Skip if we already used a similar chunk from this doc in windowing
            doc_id = r.get('document_id')
            idx = r.get('chunk_index')
            if doc_id and idx is not None:
                if doc_id in used_doc_chunks and idx in used_doc_chunks[doc_id]:
                    continue
            
            header = f"### MANBA ({'FAQ' if r['source_type'] == 'faq' else 'HUJJAT'}): {r['title']}"
            content = r['text']
            
            if r['source_type'] == 'document' and doc_id and idx is not None:
                try:
                    from chatbot_app.models import DocumentChunk
                    idx = int(idx)
                    
                    # Windowing: fetch neighbours
                    neighbors = DocumentChunk.objects.filter(
                        document_id=doc_id,
                        chunk_index__in=[idx-1, idx+1]
                    ).order_by('chunk_index')
                    
                    parts = []
                    neighbor_indices = []
                    
                    prev_chunk = neighbors.filter(chunk_index=idx-1).first()
                    if prev_chunk: 
                        parts.append(prev_chunk.chunk_text)
                        neighbor_indices.append(idx-1)
                    
                    parts.append(content)
                    neighbor_indices.append(idx)
                    
                    next_chunk = neighbors.filter(chunk_index=idx+1).first()
                    if next_chunk: 
                        parts.append(next_chunk.chunk_text)
                        neighbor_indices.append(idx+1)
                    
                    if len(parts) > 1:
                        content = "\n... (davomi) ...\n".join(parts)
                    
                    # Mark these as used to avoid double context
                    if doc_id not in used_doc_chunks: used_doc_chunks[doc_id] = []
                    used_doc_chunks[doc_id].extend(neighbor_indices)
                    
                except Exception as e:
                    logger.warning(f"Windowing error: {e}")
            
            context_blocks.append(f"{header}\n{content}")
        
        context = "\n\n" + "\n\n---\n\n".join(context_blocks) + "\n\n"
        sources = [{
            'title': r['title'],
            'category': r['category'],
            'source_type': r['source_type'],
            'relevance': round(r['confidence'] * 100),
            'faq_id': r.get('faq_id'),
            'document_id': r.get('document_id')
        } for r in sorted_top_results]
        
        return {
            'context': context,
            'top_confidence': top_results[0]['confidence'] if top_results else 0,
            'sources': sources,
            'total_found': len(merged_results)
        }

    def retrieve_with_self_correction(self, question: str, lang_code: str = 'uz', top_k: int = 5, max_iterations: int = 3) -> Dict[str, Any]:
        """Advanced RAG v5.0: LangGraph-style Self-Correction workflow"""
        refinement_history = []
        current_question = question
        
        for iteration in range(max_iterations):
            logger.info(f"üîÑ Iteration {iteration + 1}/{max_iterations}")
            retrieval_result = self.retrieve_with_sources(current_question, lang_code, top_k)
            
            if retrieval_result['sources']:
                hierarchy_result = self.faq_hierarchy.resolve_conflict(retrieval_result['sources'], current_question, lang_code)
                if hierarchy_result['conflict_detected']:
                    logger.info(f"‚ö†Ô∏è {hierarchy_result['resolution_reason']}")
            
            grading_result = self.grader.grade(current_question, retrieval_result['context'], retrieval_result.get('intent'))
            refinement_history.append({'iteration': iteration + 1, 'is_relevant': grading_result['is_relevant'], 'confidence': grading_result['confidence']})
            
            if grading_result['is_relevant']:
                logger.info(f"‚úÖ Relevant ({grading_result['confidence']:.0%})")
                return {'context': retrieval_result['context'], 'sources': retrieval_result['sources'], 'grading_result': grading_result, 'iterations_used': iteration + 1, 'refinement_history': refinement_history}
            
            if iteration < max_iterations - 1:
                current_question = grading_result['suggested_refinement']
                logger.info(f"üîÅ Refining: {current_question}")
        
        return {'context': retrieval_result['context'], 'sources': retrieval_result['sources'], 'grading_result': grading_result, 'iterations_used': max_iterations, 'refinement_history': refinement_history}





_rag_service = None

def get_rag_service() -> RAGService:
    global _rag_service
    if _rag_service is None:
        _rag_service = RAGService()
    return _rag_service

def sync_faqs_to_chromadb():
    service = get_rag_service()
    return service.sync_from_database()

    def retrieve_with_self_correction(self, question: str, lang_code: str = 'uz', top_k: int = 5, max_iterations: int = 3) -> Dict[str, Any]:
        """Advanced RAG v5.0: LangGraph-style Self-Correction workflow"""
        refinement_history = []
        current_question = question
        
        for iteration in range(max_iterations):
            logger.info(f"üîÑ Iteration {iteration + 1}/{max_iterations}")
            retrieval_result = self.retrieve_with_sources(current_question, lang_code, top_k)
            
            if retrieval_result['sources']:
                hierarchy_result = self.faq_hierarchy.resolve_conflict(retrieval_result['sources'], current_question, lang_code)
                if hierarchy_result['conflict_detected']:
                    logger.info(f"‚ö†Ô∏è {hierarchy_result['resolution_reason']}")
            
            grading_result = self.grader.grade(current_question, retrieval_result['context'], retrieval_result.get('intent'))
            refinement_history.append({'iteration': iteration + 1, 'is_relevant': grading_result['is_relevant'], 'confidence': grading_result['confidence']})
            
            if grading_result['is_relevant']:
                logger.info(f"‚úÖ Relevant ({grading_result['confidence']:.0%})")
                return {'context': retrieval_result['context'], 'sources': retrieval_result['sources'], 'grading_result': grading_result, 'iterations_used': iteration + 1, 'refinement_history': refinement_history}
            
            if iteration < max_iterations - 1:
                current_question = grading_result['suggested_refinement']
                logger.info(f"üîÅ Refining: {current_question}")
        
        return {'context': retrieval_result['context'], 'sources': retrieval_result['sources'], 'grading_result': grading_result, 'iterations_used': max_iterations, 'refinement_history': refinement_history}


_rag_service = None

def get_rag_service() -> RAGService:
    global _rag_service
    if _rag_service is None:
        _rag_service = RAGService()
    return _rag_service

def sync_faqs_to_chromadb():
    service = get_rag_service()
    return service.sync_from_database()
